{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3beb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo librerias necesarias\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np # librería para poder usar arreglos\n",
    "import pandas as pd # librería para poder usad DataFrames (tablas)\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt # to plot\n",
    "import seaborn as sns # to plot\n",
    "import sklearn # librería para poder usar modelos de machine learning\n",
    "import os\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c85414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.decomposition as sk # to compute PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ae32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Funcion para obtener los datos del archivo covid.csv\n",
    "    \"\"\"\n",
    "\n",
    "    dataSet = pd.read_csv(\"covid.csv\", header=0)\n",
    "    dataSet.set_index('date', inplace=True)\n",
    "    dataSet.index = pd.to_datetime(dataSet.index,format='%d/%m/%y',errors='ignore')\n",
    "\n",
    "    return dataSet\n",
    "\n",
    "def infoData(dataSet):\n",
    "    \"\"\"\n",
    "    Funcion para obtener datos generales\n",
    "    \"\"\"\n",
    "\n",
    "    print(dataSet.head(10))\n",
    "    print(\"\")\n",
    "    print(dataSet.info())\n",
    "    print(\"\")\n",
    "    print(dataSet.isnull().sum())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89640f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()\n",
    "infoData(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(dataSet):\n",
    "    \"\"\"\n",
    "    Funcion para visualizar la evolución de casos de Covid-19 en los estados de México usando un mapa de calor interactivo con plotly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Asegurarse de que el índice es de tipo fecha y resetearlo para tenerlo como columna\n",
    "    df = dataSet.copy()\n",
    "    df = df.reset_index()\n",
    "    df.rename(columns={'date': 'Fecha'}, inplace=True)\n",
    "\n",
    "    # Lista de estados (todas las columnas excepto 'Fecha' y 'Nacional')\n",
    "    estados = [col for col in df.columns if col not in ['Fecha', 'Nacional']]\n",
    "\n",
    "    # Convertir el dataframe a formato largo para plotly\n",
    "    df_long = df.melt(id_vars=['Fecha'], value_vars=estados, var_name='Estado', value_name='Casos')\n",
    "\n",
    "    # Cargar geojson de México (estados)\n",
    "    # Puedes descargar un geojson de los estados de México, por ejemplo desde: https://raw.githubusercontent.com/angelnmara/geojson/master/mexicoHigh.json\n",
    "    # Aquí se asume que el archivo se llama 'mexico_estados.geojson' y está en el mismo directorio\n",
    "    with open('mexico_estados.geojson', 'r', encoding='utf-8') as f:\n",
    "        mexico_geojson = json.load(f)\n",
    "\n",
    "    # Crear el mapa animado\n",
    "    fig = px.choropleth(\n",
    "        df_long,\n",
    "        geojson=mexico_geojson,\n",
    "        locations='Estado',\n",
    "        color='Casos',\n",
    "        animation_frame='Fecha',\n",
    "        featureidkey='properties.NOM_ENT',  # Depende del geojson, puede ser 'properties.NOM_ENT' o 'properties.name'\n",
    "        color_continuous_scale='Reds',\n",
    "        scope='world',\n",
    "        labels={'Casos': 'Casos normalizados'},\n",
    "        title='Evolución de casos de Covid-19 por estado en México'\n",
    "    )\n",
    "    fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1760e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running `%pip install nbformat` in a Jupyter notebook should install the `nbformat` package. However, if you still see the error `ValueError: Mime type rendering requires nbformat>=4.2.0 but it is not installed`, possible reasons include:\n",
    "\n",
    "#1. **Kernel Restart Needed:** After installing a new package, you often need to restart the Jupyter kernel for the changes to take effect.\n",
    "#2. **Multiple Python Environments:** The notebook kernel might be using a different Python environment than the one where `nbformat` was installed.\n",
    "#3. **Installation Failed:** The installation may have failed silently or with a warning.\n",
    "\n",
    "#**How to fix:**\n",
    "#- Run `%pip show nbformat` in a cell to check if it is installed and its version.\n",
    "#- If not installed, run `%pip install nbformat --upgrade` and then restart the kernel.\n",
    "#- Make sure your notebook is running in the same environment where you installed `nbformat`.\n",
    "\n",
    "#If the problem persists, check your Jupyter server logs for more details.\n",
    "map(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataSet):\n",
    "    \"\"\"\n",
    "    Funcion para el preprocesamiento de datos\n",
    "    \"\"\"\n",
    "\n",
    "    local_dataSet = dataSet.dropna()\n",
    "    columns_list = dataSet.columns.tolist()\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    min_max_scaler.fit(local_dataSet)\n",
    "    new_dataSet = min_max_scaler.transform(local_dataSet)\n",
    "\n",
    "    new_dataSet = pd.DataFrame(np.round(new_dataSet,4), columns=columns_list)\n",
    "    new_dataSet.index = local_dataSet.index\n",
    "\n",
    "    return new_dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessData = preprocess_data(data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(preprocessData['Nacional'])\n",
    "plt.title('Datos nacional', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb783d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_Transformation(dataSet):\n",
    "    features = dataSet\n",
    "    features = features.drop(['Nacional'], axis=1)\n",
    "\n",
    "    # principal components analysis\n",
    "    pca = sk.PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(features)\n",
    "    principalComp = pd.DataFrame(data = principalComponents,\n",
    "                                 columns = ['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3'])\n",
    "    principalComp.set_index(dataSet.index,inplace = True, drop=True)\n",
    "    principalComp['Nacional'] = dataSet['Nacional']\n",
    "    cols = principalComp.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    principalComp = principalComp[cols]\n",
    "\n",
    "    print(\"==========================================================\")\n",
    "    print(\"                 PCA ANALYSIS\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(principalComp)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    varRatio = (pca.explained_variance_ratio_)*100\n",
    "    data = {'P. Comp. 1':varRatio[0],'P. Comp. 2':varRatio[1],'P. Comp. 3':varRatio[2]}\n",
    "    dfVar = pd.DataFrame(data, index=['Exp Var Ratio %'])\n",
    "    print(dfVar)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return principalComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaData = pca_Transformation(preprocessData)\n",
    "pcaData.head(1215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb01954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(pcaData, test_size=0.25, shuffle=False)\n",
    "print(train_data.head(1215))\n",
    "print(\"\")\n",
    "print(len(train_data))\n",
    "print(\"\")\n",
    "print(test_data.head(1215))\n",
    "print(\"\")\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b80226",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_data)\n",
    "plt.title('TRAIN DATA', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(test_data)\n",
    "plt.title('TEST DATA', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64843ce3",
   "metadata": {},
   "source": [
    "Modelado del PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22937b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Definición de variables (X e Y) anteriormente realizada ===\n",
    "# X_train = train_data.iloc[:, 0:3]\n",
    "# Y_train = train_data.iloc[:, [-1]]\n",
    "\n",
    "# X_test = test_data.iloc[:, 0:3]\n",
    "# Y_test = test_data.iloc[:, [-1]]\n",
    "\n",
    "# === Definición de variables (X e Y) Train y Test===\n",
    "X_train = train_data[['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3']]  # Características\n",
    "Y_train = train_data[['Nacional']]  # Objetivo\n",
    "X_test = test_data[['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3']]  # Características\n",
    "Y_test = test_data[['Nacional']]  # Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39203635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "dt_regr = DecisionTreeRegressor()\n",
    "param_search_dt_regr = {\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'], #Puedo añadir 'friedman_mse',\n",
    "    'max_depth': [3, 5, 7, 9, 15, 20, None]\n",
    "}\n",
    "# Puedo probar con'min_samples_split': [2, 5, 10],\n",
    "# Puedo probar con'min_samples_leaf': [1, 2, 4]\n",
    "\n",
    "# Utilizo GridSearchCV para encontrar los mejores parámetros\n",
    "# Definir X (característica) y y (objetivo)\n",
    "X = train_data[['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3']]\n",
    "y = train_data[['Nacional']]\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=dt_regr, param_grid=param_search_dt_regr, n_jobs=-1, cv=5, verbose=1)\n",
    "grid_search_dt.fit(X, y)  # Entreno el modelo con los componentes principales como característica y la columna 'Nacional' como objetivo\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "\n",
    "print(\"==========================================================\")\n",
    "print(\"Mejores parámetros para Decision Tree Regressor:\")\n",
    "print(best_params_dt)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d252024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_optimo = grid_search_dt.best_estimator_\n",
    "# === MultiOutputRegressor para predecir los 3 componentes principales ===\n",
    "dt_optimo.fit(X_train, Y_train)\n",
    "\n",
    "# === Predicciones ===\n",
    "train_pred_dt = dt_optimo.predict(X_train)  # Predecir 'Nacional' para train\n",
    "test_pred_dt = dt_optimo.predict(X_test)    # Predecir 'Nacional' para test\n",
    "\n",
    "# === Convertir a DataFrames para análisis ===\n",
    "pred_train_df_dt = pd.DataFrame(train_pred_dt, columns=[\"Nacional\"], index=X_train.index)\n",
    "pred_test_df_dt = pd.DataFrame(test_pred_dt, columns=[\"Nacional\"], index=X_test.index)\n",
    "\n",
    "pred_train_df_dt.columns = [\"Nacional\"]\n",
    "pred_test_df_dt.columns = [\"Nacional\"]\n",
    "\n",
    "def evaluacion_metricas(true_df, pred_df):\n",
    "    \"\"\"\n",
    "    Calculate various regression metrics comparing true and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_df : pandas.DataFrame\n",
    "        DataFrame containing actual values with 'Nacional' column\n",
    "    pred_df : pandas.DataFrame\n",
    "        DataFrame containing predicted values with 'Nacional' column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing calculated metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics dictionary\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Get sample size and number of predictors\n",
    "    n = len(true_df[\"Nacional\"])\n",
    "    k = 1  # number of predictors\n",
    "    \n",
    "    # Calculate basic error metrics\n",
    "    mse = mean_squared_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mae = mean_absolute_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mape = mean_absolute_percentage_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    r2 = r2_score(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # Calculate information criteria\n",
    "    if mse > 0:\n",
    "        log_likelihood = -n / 2 * np.log(2 * np.pi * mse) - n / 2\n",
    "        aic = 2 * k - 2 * log_likelihood\n",
    "        bic = np.log(n) * k - 2 * log_likelihood\n",
    "    else:\n",
    "        aic = np.nan\n",
    "        bic = np.nan\n",
    "    \n",
    "    # Calculate Amemiya's Prediction Criterion\n",
    "    apc = (1 + (k / n)) * mse\n",
    "    \n",
    "    # Store all metrics in dictionary\n",
    "    metrics_dict[\"Nacional\"] = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"R2\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"AIC\": aic,\n",
    "        \"APC\": apc,\n",
    "        \"BIC\": bic\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(metrics_dict).T\n",
    "# Usar train_data[[\"Nacional\"]] y test_data[[\"Nacional\"]] para las métricas\n",
    "train_metrics_dt = evaluacion_metricas(train_data[[\"Nacional\"]], pred_train_df_dt)\n",
    "test_metrics_dt = evaluacion_metricas(test_data[[\"Nacional\"]], pred_test_df_dt)\n",
    "\n",
    "print(\"\\n Métrica - Train Nacional:\")\n",
    "print(train_metrics_dt)\n",
    "print(\"\\n Métrica - Test Nacional:\")\n",
    "print(test_metrics_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafica de la predicción vs real DT\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(Y_test.index, Y_test.values, label=\"Real\", linewidth=2)  \n",
    "plt.plot(pred_test_df_dt.index, pred_test_df_dt.values, label=\"Predicción\", linestyle=\"--\")  \n",
    "plt.title(\"Predicción vs Real - Nacional DT\", fontsize=12, pad=20)\n",
    "plt.xlabel(\"Tiempo\", fontsize=10)\n",
    "plt.ylabel(\"Valor\", fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "rf_regr = RandomForestRegressor()\n",
    "param_search_rf_regr = {\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'], #Puedo añadir 'friedman_mse',\n",
    "    'n_estimators': [10, 50, 100, 200, None], # Puedo probar con [10, 50, 100, 200, 500]\n",
    "    'max_depth': [3, 5, 7, 9, 15, 20, None]\n",
    "}\n",
    "\n",
    "# Utilizo GridSearchCV para encontrar los mejores parámetros\n",
    "# Definir X (característica) y y (objetivo)\n",
    "X = train_data[['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3']]\n",
    "y = train_data[['Nacional']]\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=rf_regr, param_grid=param_search_rf_regr, n_jobs=-1, cv=5, verbose=1)\n",
    "grid_search_rf.fit(X, y)  # Entreno el modelo con la columna 'Nacional' como característica y los componentes principales como objetivo\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "print(\"==========================================================\")\n",
    "print(\"Mejores parámetros para Decision Tree Regressor:\")\n",
    "print(best_params_rf)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ca228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optimo = grid_search_rf.best_estimator_\n",
    "# === Ajustar el modelo SVM con los datos de entrenamiento ===\n",
    "rf_optimo.fit(X_train, Y_train)\n",
    "\n",
    "# === Predicciones ===\n",
    "train_pred_rf = rf_optimo.predict(X_train)  # Predecir 'Nacional' para train\n",
    "test_pred_rf = rf_optimo.predict(X_test)    # Predecir 'Nacional' para test\n",
    "\n",
    "# === Convertir a DataFrames para análisis ===\n",
    "pred_train_df_rf = pd.DataFrame(train_pred_rf, columns=[\"Nacional\"], index=Y_train.index)\n",
    "pred_test_df_rf = pd.DataFrame(test_pred_rf, columns=[\"Nacional\"], index=Y_test.index)\n",
    "\n",
    "pred_train_df_rf.columns = [\"Nacional\"]\n",
    "pred_test_df_rf.columns = [\"Nacional\"]\n",
    "\n",
    "def evaluacion_metricas(true_df, pred_df):\n",
    "    \"\"\"\n",
    "    Calculate various regression metrics comparing true and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_df : pandas.DataFrame\n",
    "        DataFrame containing actual values with 'Nacional' column\n",
    "    pred_df : pandas.DataFrame\n",
    "        DataFrame containing predicted values with 'Nacional' column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing calculated metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics dictionary\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Get sample size and number of predictors\n",
    "    n = len(true_df[\"Nacional\"])\n",
    "    k = 1  # number of predictors\n",
    "    \n",
    "    # Calculate basic error metrics\n",
    "    mse = mean_squared_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mae = mean_absolute_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mape = mean_absolute_percentage_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    r2 = r2_score(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # Calculate information criteria\n",
    "    if mse > 0:\n",
    "        log_likelihood = -n / 2 * np.log(2 * np.pi * mse) - n / 2\n",
    "        aic = 2 * k - 2 * log_likelihood\n",
    "        bic = np.log(n) * k - 2 * log_likelihood\n",
    "    else:\n",
    "        aic = np.nan\n",
    "        bic = np.nan\n",
    "    \n",
    "    # Calculate Amemiya's Prediction Criterion\n",
    "    apc = (1 + (k / n)) * mse\n",
    "    \n",
    "    # Store all metrics in dictionary\n",
    "    metrics_dict[\"Nacional\"] = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"R2\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"AIC\": aic,\n",
    "        \"APC\": apc,\n",
    "        \"BIC\": bic\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(metrics_dict).T\n",
    "# Usar train_data[[\"Nacional\"]] y test_data[[\"Nacional\"]] para las métricas\n",
    "train_metrics_rf = evaluacion_metricas(train_data[[\"Nacional\"]], pred_train_df_rf)\n",
    "test_metrics_rf = evaluacion_metricas(test_data[[\"Nacional\"]], pred_test_df_rf)\n",
    "\n",
    "print(\"\\n Métrica - Train Nacional:\")\n",
    "print(train_metrics_rf)\n",
    "print(\"\\n Métrica - Test Nacional:\")\n",
    "print(test_metrics_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafica de la predicción vs real RF\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(Y_test.index, Y_test.values, label=\"Real\", linewidth=2)  \n",
    "plt.plot(pred_test_df_rf.index, pred_test_df_rf.values, label=\"Predicción\", linestyle=\"--\")  \n",
    "plt.title(\"Predicción vs Real - Nacional RF\", fontsize=12, pad=20)\n",
    "plt.xlabel(\"Tiempo\", fontsize=10)\n",
    "plt.ylabel(\"Valor\", fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm_regr = svm.SVR()\n",
    "param_search_svm_regr = {\n",
    "    'kernel': ['linear'],# , 'poly', 'rbf', 'sigmoid'\n",
    "    'degree': [2],  # 3, 4, 5, 6\n",
    "    'C': [0.001, 0.1, 1, 10, 100], # Puedo probar con [0.001, 0.01, 0.1, 1]\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'epsilon': [0.1, 0.2, 0.5, 1],\n",
    "}\n",
    "# Puedo agregar: 'estimator__gamma': ['scale', 'auto'], # Puedo probar con [0.001, 0.01, 0.1, 1]\n",
    "# Puedo agregar: 'estimator__epsilon': [0.1, 0.2, 0.5, 1]\n",
    "\n",
    "# Utilizo GridSearchCV para encontrar los mejores parámetros\n",
    "# Definir X (característica) y y (objetivo)\n",
    "X = train_data[['P. Comp. 1', 'P. Comp. 2', 'P. Comp. 3']]\n",
    "y = train_data[['Nacional']]\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator=svm_regr, param_grid=param_search_svm_regr, n_jobs=-1, cv=5, verbose=1)\n",
    "grid_search_svm.fit(X, y)  # Entreno el modelo con los componentes principales como característica y la columna 'Nacional' como objetivo\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "\n",
    "print(\"==========================================================\")\n",
    "print(\"Mejores parámetros para Support Vector Machine Regressor:\")\n",
    "print(best_params_svm)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_optimo = grid_search_svm.best_estimator_\n",
    "# === Ajustar el modelo SVM con los datos de entrenamiento ===\n",
    "svm_optimo.fit(X_train, Y_train)\n",
    "\n",
    "# === Predicciones ===\n",
    "train_pred_svm = svm_optimo.predict(X_train)  # Predecir 'Nacional' para train\n",
    "test_pred_svm = svm_optimo.predict(X_test)    # Predecir 'Nacional' para test\n",
    "\n",
    "# === Convertir a DataFrames para análisis ===\n",
    "pred_train_df_svm = pd.DataFrame(train_pred_svm, columns=[\"Nacional\"], index=X_train.index)\n",
    "pred_test_df_svm = pd.DataFrame(test_pred_svm, columns=[\"Nacional\"], index=X_test.index)\n",
    "\n",
    "pred_train_df_svm.columns = [\"Nacional\"]\n",
    "pred_test_df_svm.columns = [\"Nacional\"]\n",
    "\n",
    "def evaluacion_metricas(true_df, pred_df):\n",
    "    \"\"\"\n",
    "    Calculate various regression metrics comparing true and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_df : pandas.DataFrame\n",
    "        DataFrame containing actual values with 'Nacional' column\n",
    "    pred_df : pandas.DataFrame\n",
    "        DataFrame containing predicted values with 'Nacional' column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing calculated metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics dictionary\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Get sample size and number of predictors\n",
    "    n = len(true_df[\"Nacional\"])\n",
    "    k = 1  # number of predictors\n",
    "    \n",
    "    # Calculate basic error metrics\n",
    "    mse = mean_squared_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mae = mean_absolute_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    mape = mean_absolute_percentage_error(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    r2 = r2_score(true_df[\"Nacional\"], pred_df[\"Nacional\"])\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # Calculate information criteria\n",
    "    if mse > 0:\n",
    "        log_likelihood = -n / 2 * np.log(2 * np.pi * mse) - n / 2\n",
    "        aic = 2 * k - 2 * log_likelihood\n",
    "        bic = np.log(n) * k - 2 * log_likelihood\n",
    "    else:\n",
    "        aic = np.nan\n",
    "        bic = np.nan\n",
    "    \n",
    "    # Calculate Amemiya's Prediction Criterion\n",
    "    apc = (1 + (k / n)) * mse\n",
    "    \n",
    "    # Store all metrics in dictionary\n",
    "    metrics_dict[\"Nacional\"] = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"R2\": r2,\n",
    "        \"RMSE\": rmse,\n",
    "        \"AIC\": aic,\n",
    "        \"APC\": apc,\n",
    "        \"BIC\": bic\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(metrics_dict).T\n",
    "# Usar train_data[[\"Nacional\"]] y test_data[[\"Nacional\"]] para las métricas\n",
    "train_metrics_svm = evaluacion_metricas(train_data[[\"Nacional\"]], pred_train_df_svm)\n",
    "test_metrics_svm = evaluacion_metricas(test_data[[\"Nacional\"]], pred_test_df_svm)\n",
    "\n",
    "print(\"\\n Métrica - Train Nacional:\")\n",
    "print(train_metrics_svm)\n",
    "print(\"\\n Métrica - Test Nacional:\")\n",
    "print(test_metrics_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db867c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafica de la predicción vs real SVM\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(Y_test.index, Y_test.values, label=\"Real\", linewidth=2)  \n",
    "plt.plot(pred_test_df_svm.index, pred_test_df_svm.values, label=\"Predicción\", linestyle=\"--\")  \n",
    "plt.title(\"Predicción vs Real - Nacional SVM\", fontsize=12, pad=20)\n",
    "plt.xlabel(\"Tiempo\", fontsize=10)\n",
    "plt.ylabel(\"Valor\", fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
